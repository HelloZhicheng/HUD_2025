---
title: "Multinomial Logistic regression (MLR)"
output: MLR
---

```{python}
### Explanation:
1. **Dataset Loading and Preparation**:
   - The dataset is loaded and the response variable is converted into a factor for multinomial regression.
   - Predictor variables are prepared as a matrix since `glmnet` expects this format.

2. **Model Training and Validation**:
   - The data is split into training (70%) and testing (30%) sets for evaluation.
   - Cross-validation is performed to find the optimal `lambda` value for the regularization parameter.

3. **Bootstrap Analysis**:
   - Bootstrap resampling is used to assess variability and compute standard errors for the coefficients.
   - Z-values and p-values are calculated to evaluate the significance of the predictors.

4. **Model Evaluation**:
   - Accuracy is calculated from predictions on the test set.
   - Deviance ratio and model coefficients are analyzed for goodness of fit.
```

```{r}
# Load necessary libraries
library(glmnet)
library(dplyr)

# Load the dataset
mydata <- read.csv("cluster_weighted_0617_3cluster_mean.csv", header = TRUE)
```

```{r}
# Convert the target variable into a factor for multinomial regression
mydata$type <- as.factor(mydata$Kmeans_t)

# Check the structure of the dataset to ensure correct data types
str(mydata)
```

```{r}
# Split the dataset into training and testing sets (70% train, 30% test)
set.seed(123) # Set a seed for reproducibility
train_index <- sample(1:nrow(mydata), 0.7 * nrow(mydata))
train_data <- mydata[train_index, ]
test_data <- mydata[-train_index, ]

# Prepare the predictor variables (x) and the response variable (y)
x <- as.matrix(mydata %>% select(exp_mean, sen_mean, m_mean, pre_mean, rec_mean, res_mean))
y <- as.factor(mydata$type)

# Subset predictor and response variables for the training data
x_train <- as.matrix(train_data %>% select(exp_mean, sen_mean, m_mean, pre_mean, rec_mean, res_mean))
y_train <- as.factor(train_data$type)

# Prepare the testing dataset for prediction
x_test <- as.matrix(test_data %>% select(exp_mean, sen_mean, m_mean, pre_mean, rec_mean, res_mean))
y_test <- as.factor(test_data$type)

# Perform cross-validation to find the optimal lambda value for multinomial regression
fit <- cv.glmnet(x, y, family = "multinomial", alpha = 0) # alpha = 0 for ridge regression
best_lambda <- fit$lambda.min # Optimal lambda minimizing cross-validation error
print(best_lambda)

# Fit the multinomial regression model using the optimal lambda value
fit <- glmnet(x_train, y_train, family = "multinomial", alpha = 0, lambda = best_lambda)

# Predict the class labels for the testing dataset
predictions <- predict(fit, newx = x_test, s = "lambda.min", type = "class")

# Compute the confusion matrix and calculate the accuracy
actual <- test_data$type
confusion_matrix <- table(actual, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)

# Fit the final model using the entire dataset and the optimal lambda value
final_model <- glmnet(x, y, family = "multinomial", alpha = 0, lambda = best_lambda)

# Print the coefficients of the final model
print(coef(final_model, s = best_lambda))
```

```{r}
# Check the deviance ratio (explained variation by the model)
final_model$dev.ratio

# Perform bootstrap resampling to estimate coefficient variability
n_bootstrap <- 1000 # Number of bootstrap samples

# Function to extract coefficients from glmnet object
extract_coefs <- function(fit) {
  coef_list <- coef(fit, s = fit$lambda)
  coefs <- do.call(cbind, lapply(coef_list, as.matrix))
  return(as.numeric(coefs[-1, ])) # Exclude the intercept term
}

# Bootstrap procedure
boot_coefs <- replicate(n_bootstrap, {
  indices <- sample(1:nrow(x), replace = TRUE) # Resample with replacement
  x_boot <- x[indices, ]
  y_boot <- y[indices]
  lasso_fit_boot <- glmnet(x_boot, y_boot, family = "multinomial", alpha = 0, lambda = best_lambda)
  extract_coefs(lasso_fit_boot)
})

# Compute standard errors from bootstrap results
boot_coefs_matrix <- matrix(unlist(boot_coefs), ncol = n_bootstrap)
standard_errors <- apply(boot_coefs_matrix, 1, sd)

# Extract coefficients from the final model
final_coefs <- as.numeric(do.call(cbind, lapply(coef(final_model, s = best_lambda), as.matrix))[-1, ])

# Calculate z-values
z_values <- final_coefs / standard_errors

# Convert z-values to p-values
p_values <- 2 * pnorm(-abs(z_values))  # Two-tailed test

# Print the p-values
print(p_values)

# Format p-values for better readability
formatted_p_values <- formatC(p_values, format = "f", digits = 4)
print(formatted_p_values)
```

