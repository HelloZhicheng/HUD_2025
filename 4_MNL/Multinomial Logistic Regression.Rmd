---
title: "Multinomial Logistic regression (MLR)"
Description: "This script performs multinomial logistic regression using the `glmnet` package, including data splitting, hyperparameter tuning, model evaluation, and bootstrap resampling for coefficient uncertainty estimation."
---

```{r}
# Load required libraries
library(glmnet)
library(dplyr)
```

```{r}
# Step 1: Load the data
# Replace "Cluster_result.csv" with the correct path to your dataset.
mydata <- read.csv("Cluster_result.csv", header = TRUE)
```

```{r}
# Step 2: Convert target variable to a factor
mydata$type <- as.factor(mydata$Kmeans_t)
```

```{r}
# Step 3: Check the structure of the dataset
str(mydata)
```

```{r}
# Step 4: Split the data into training (70%) and testing (30%) sets
set.seed(123) # Set a seed for reproducibility
train_index <- sample(1:nrow(mydata), 0.7 * nrow(mydata))
train_data <- mydata[train_index, ]
test_data <- mydata[-train_index, ]
```

```{r}
# Step 5: Prepare predictor (X) and response (Y) matrices
x <- as.matrix(mydata %>% select(exp_mean, sen_mean, m_mean, pre_mean, rec_mean, res_mean))
y <- as.factor(mydata$type)

# Split for training data
x_train <- as.matrix(train_data %>% select(exp_mean, sen_mean, m_mean, pre_mean, rec_mean, res_mean))
y_train <- as.factor(train_data$type)
```

```{r}
# Step 6: Perform cross-validation to determine the best lambda
fit <- cv.glmnet(x, y, family = "multinomial", alpha = 0)  # Ridge regression (alpha = 0)
best_lambda <- fit$lambda.min  # Optimal lambda value
print(paste("Best lambda:", best_lambda))
```

```{r}
# Step 7: Fit the final model using the best lambda
fit <- glmnet(x_train, y_train, family = "multinomial", alpha = 0, lambda = best_lambda)
```

```{r}
# Step 8: Make predictions on the test data
x_test <- as.matrix(test_data %>% select(exp_mean, sen_mean, m_mean, pre_mean, rec_mean, res_mean))
y_test <- as.factor(test_data$type)
predictions <- predict(fit, newx = x_test, s = "lambda.min", type = "class")
```

```{r}
# Step 9: Evaluate the model using a confusion matrix
actual <- test_data$type
confusion_matrix <- table(actual, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", accuracy))
```

```{r}
# Step 10: Extract coefficients from the final model
final_model <- glmnet(x, y, family = "multinomial", alpha = 0, lambda = best_lambda)
print(coef(final_model, s = best_lambda))  # Coefficients for the final model
```

```{r}
# Step 11: Calculate model deviance ratio
deviance_ratio <- final_model$dev.ratio
print(paste("Deviance Ratio:", deviance_ratio))
```

```{r}
# Step 12: Bootstrap resampling to estimate coefficient uncertainty
n_bootstrap <- 1000  # Number of bootstrap samples

# Function to extract coefficients from a glmnet model
extract_coefs <- function(fit) {
  coef_list <- coef(fit, s = fit$lambda)
  coefs <- do.call(cbind, lapply(coef_list, as.matrix))
  return(as.numeric(coefs[-1, ]))  # Exclude intercept
}

# Bootstrap procedure
boot_coefs <- replicate(n_bootstrap, {
  indices <- sample(1:nrow(x), replace = TRUE)  # Resample with replacement
  x_boot <- x[indices, ]
  y_boot <- y[indices]
  lasso_fit_boot <- glmnet(x_boot, y_boot, family = "multinomial", alpha = 0, lambda = best_lambda)
  extract_coefs(lasso_fit_boot)
})
```

```{r}
# Step 13: Compute standard errors from bootstrap results
boot_coefs_matrix <- matrix(unlist(boot_coefs), ncol = n_bootstrap)
standard_errors <- apply(boot_coefs_matrix, 1, sd)

# Extract coefficients from the final model
final_coefs <- as.numeric(do.call(cbind, lapply(coef(final_model, s = best_lambda), as.matrix))[-1, ])

# Calculate z-values
z_values <- final_coefs / standard_errors

# Convert z-values to p-values (two-tailed test)
p_values <- 2 * pnorm(-abs(z_values))  
print("P-values:")
print(formatC(p_values, format = "f", digits = 4))
```

